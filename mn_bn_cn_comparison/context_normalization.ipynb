{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q84qOQxZEUD"
   },
   "source": [
    "# **Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zp_uCmldZ-ZZ"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow-addons to have access for some optimizers like AdamW\n",
    "! pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhaZNgBhY6Vr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import cifar100, cifar10\n",
    "import tensorflow_datasets as tfds\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.random import rand\n",
    "from pylab import figure\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy, Precision, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# Import the context normalization layer\n",
    "import os\n",
    "import sys\n",
    "package_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(package_dir)\n",
    "sys.path.append(root_dir)\n",
    "from normalization.layers import ContextNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vSCq0zhaiG4"
   },
   "source": [
    "# **Define some functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diAriSbpZo4g"
   },
   "outputs": [],
   "source": [
    "# Estimate the mean and variance in a given image dataset along channels axis\n",
    "def compute_mean_std(dataset):\n",
    "    data_r = np.dstack([dataset[i][:, :, 0] for i in range(len(dataset))])\n",
    "    data_g = np.dstack([dataset[i][:, :, 1] for i in range(len(dataset))])\n",
    "    data_b = np.dstack([dataset[i][:, :, 2] for i in range(len(dataset))])\n",
    "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
    "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWvrm3-_ayl-"
   },
   "outputs": [],
   "source": [
    "# Define a Data Augmentation Layer for CIFAR-10 and CIFAR-100\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Resizing(72, 72),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTjASZi0bNwn"
   },
   "outputs": [],
   "source": [
    "# Define functions to save lists and load files\n",
    "def write_list(a_list, file_name):\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "def read_list(file_name):\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBAankDVbmHF"
   },
   "source": [
    "# **CIFAR-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVnAFLpxbolG"
   },
   "outputs": [],
   "source": [
    "# Define constant parameters\n",
    "class CFG:\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 10\n",
    "    num_contexts = 3\n",
    "    num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oit6jDGb3Wy"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Encod labels with one-hot representation\n",
    "y_train_sparse = to_categorical(y_train, num_classes=CFG.num_classes)\n",
    "y_test_sparse = to_categorical(y_test, num_classes=CFG.num_classes)\n",
    "\n",
    "# Standardize the dataset using the defined function computer_mean_std\n",
    "mean, std = compute_mean_std(x_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY1UX1n9cNMr"
   },
   "outputs": [],
   "source": [
    "# Load labels generated by the GMM algorithms\n",
    "gmm_train_labels = read_list(\"gmm/gmm_cifar10_tr_labels\")\n",
    "gmm_test_labels = read_list(\"gmm/gmm_cifar10_ts_labels\")\n",
    "\n",
    "context_train = [[0]*CFG.num_contexts for _ in range(len(gmm_train_labels))]\n",
    "context_test = [[0]*CFG.num_contexts for _ in range(len(gmm_test_labels))]\n",
    "\n",
    "for iter, label in enumerate(gmm_train_labels):\n",
    "    context_train[iter][label] = 1\n",
    "\n",
    "for iter, label in enumerate(gmm_test_labels):\n",
    "    context_test[iter][label] = 1\n",
    "\n",
    "context_train = np.array(context_train)\n",
    "context_test = np.array(context_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVsq9doTcpQf"
   },
   "outputs": [],
   "source": [
    "# Build the ConvNet model\n",
    "def build_cnn(num_classes=CFG.num_classes, num_contexts=CFG.num_contexts, learning_rate=CFG.learning_rate, weight_decay=CFG.weight_decay):\n",
    "    input_image = tf.keras.layers.Input(shape=(32,32,3))\n",
    "    context_id = tf.keras.layers.Input(shape=(num_contexts,), dtype='int32')\n",
    "    augmented = data_augmentation(input_image)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(augmented)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    pool1 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool1)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool2)\n",
    "    conv3 = ContextNormalization()([conv3, context_id])\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1),  padding=\"same\")(pool3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.ReLU()(conv4)\n",
    "\n",
    "    pool4 = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(conv4)\n",
    "    flattened = tf.keras.layers.Flatten()(pool4)\n",
    "    outputs = tf.keras.layers.Dense(num_classes)(flattened)\n",
    "\n",
    "    model = tf.keras.models.Model([input_image, context_id], outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            Precision(name=\"precision\"),\n",
    "            Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByW-7g1TdaND"
   },
   "outputs": [],
   "source": [
    "# Train our model and validate it per 25 epochs\n",
    "class CustomValidationCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            self.model.load_weights(\"model.ckpt\")\n",
    "            _, accuracy, top_5_accuracy, precision, recall, f1 = self.model.evaluate([x_test, context_test], y_test_sparse)\n",
    "            print(f\"Test accuracy at epoch {epoch + 1}: {round(accuracy * 100, 2)}%\")\n",
    "            print(f\"Test top 5 accuracy at epoch {epoch + 1}: {round(top_5_accuracy * 100, 2)}%\")\n",
    "            print(f\"Precision at epoch {epoch + 1}: {round(precision * 100, 2)}%\")\n",
    "            print(f\"Recall at epoch {epoch + 1}: {round(recall * 100, 2)}%\")\n",
    "            print(f\"F1-score at epoch {epoch + 1}: {f1}%\")\n",
    "\n",
    "def run_model(model, filepath, batch_size=CFG.batch_size, num_epochs=CFG.num_epochs):\n",
    "    checkpoint_filepath = filepath\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=[x_train, context_train],\n",
    "        y=y_train_sparse,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback, CustomValidationCallback()],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = build_cnn()\n",
    "  history = run_model(model, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PP1_OT1pdsMh"
   },
   "outputs": [],
   "source": [
    "# Save metrics list on files\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LeFL53ahTY3"
   },
   "outputs": [],
   "source": [
    "# See model generalization (loss and validation loss)\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRd5VWQIuN2e"
   },
   "source": [
    "# **CIFAR-100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfSEg4bLuN2e"
   },
   "outputs": [],
   "source": [
    "# Define constant parameters\n",
    "class CFG:\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 100\n",
    "    num_contexts = 3\n",
    "    num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKetoI4KuN2f"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Encod labels with one-hot representation\n",
    "y_train_sparse = to_categorical(y_train, num_classes=CFG.num_classes)\n",
    "y_test_sparse = to_categorical(y_test, num_classes=CFG.num_classes)\n",
    "\n",
    "# Standardize the dataset using the defined function computer_mean_std\n",
    "mean, std = compute_mean_std(x_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4SeGfwhuN2f"
   },
   "outputs": [],
   "source": [
    "# Load labels generated by the GMM algorithms\n",
    "gmm_train_labels = read_list(\"gmm/gmm_cifar100_tr_labels\")\n",
    "gmm_test_labels = read_list(\"gmm/gmm_cifar100_ts_labels\")\n",
    "\n",
    "context_train = [[0]*CFG.num_contexts for _ in range(len(gmm_train_labels))]\n",
    "context_test = [[0]*CFG.num_contexts for _ in range(len(gmm_test_labels))]\n",
    "\n",
    "for iter, label in enumerate(gmm_train_labels):\n",
    "    context_train[iter][label] = 1\n",
    "\n",
    "for iter, label in enumerate(gmm_test_labels):\n",
    "    context_test[iter][label] = 1\n",
    "\n",
    "context_train = np.array(context_train)\n",
    "context_test = np.array(context_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-5vq-70uN2f"
   },
   "outputs": [],
   "source": [
    "# Build the ConvNet model\n",
    "def build_cnn(num_classes=CFG.num_classes, num_contexts=CFG.num_contexts, learning_rate=CFG.learning_rate, weight_decay=CFG.weight_decay):\n",
    "    input_image = tf.keras.layers.Input(shape=(32,32,3))\n",
    "    context_id = tf.keras.layers.Input(shape=(num_contexts,), dtype='int32')\n",
    "    augmented = data_augmentation(input_image)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(augmented)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    pool1 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool1)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool2)\n",
    "    conv3 = ContextNormalization()([conv3, context_id])\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1),  padding=\"same\")(pool3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.ReLU()(conv4)\n",
    "\n",
    "    pool4 = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(conv4)\n",
    "    flattened = tf.keras.layers.Flatten()(pool4)\n",
    "    outputs = tf.keras.layers.Dense(num_classes)(flattened)\n",
    "\n",
    "    model = tf.keras.models.Model([input_image, context_id], outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            Precision(name=\"precision\"),\n",
    "            Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0NZerNGuN2f"
   },
   "outputs": [],
   "source": [
    "# Train our model and validate it per 25 epochs\n",
    "class CustomValidationCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            self.model.load_weights(\"model.ckpt\")\n",
    "            _, accuracy, top_5_accuracy, precision, recall, f1 = self.model.evaluate([x_test, context_test], y_test_sparse)\n",
    "            print(f\"Test accuracy at epoch {epoch + 1}: {round(accuracy * 100, 2)}%\")\n",
    "            print(f\"Test top 5 accuracy at epoch {epoch + 1}: {round(top_5_accuracy * 100, 2)}%\")\n",
    "            print(f\"Precision at epoch {epoch + 1}: {round(precision * 100, 2)}%\")\n",
    "            print(f\"Recall at epoch {epoch + 1}: {round(recall * 100, 2)}%\")\n",
    "            print(f\"F1-score at epoch {epoch + 1}: {f1}%\")\n",
    "\n",
    "def run_model(model, filepath, batch_size=CFG.batch_size, num_epochs=CFG.num_epochs):\n",
    "    checkpoint_filepath = filepath\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=[x_train, context_train],\n",
    "        y=y_train_sparse,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback, CustomValidationCallback()],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = build_cnn()\n",
    "  history = run_model(model, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7A6gboKuN2f"
   },
   "outputs": [],
   "source": [
    "# Save metrics list on files\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObYUi4-ruN2g"
   },
   "outputs": [],
   "source": [
    "# See model generalization (loss and validation loss)\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmEJswsJ57O6"
   },
   "source": [
    "# **Tiny ImageNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86RYRdrP59nI"
   },
   "outputs": [],
   "source": [
    "# Clone ImageNet dataset\n",
    "! git clone https://github.com/seshuad/IMagenet\n",
    "! ls 'IMagenet/tiny-imagenet-200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrRpoKlr6obb"
   },
   "outputs": [],
   "source": [
    "# Define all parameters\n",
    "class CFG:\n",
    "    projection_dims = 256\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 200\n",
    "    num_contexts = 3\n",
    "    num_epochs = 100\n",
    "    image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JblIF00f6heO"
   },
   "outputs": [],
   "source": [
    "# Load dataset and split into train and test\n",
    "path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "def get_id_dictionary():\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open( path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "    return id_dict\n",
    "\n",
    "def get_class_to_id_dict():\n",
    "    id_dict = get_id_dictionary()\n",
    "    all_classes = {}\n",
    "    result = {}\n",
    "    for i, line in enumerate(open( path + 'words.txt', 'r')):\n",
    "        n_id, word = line.split('\\t')[:2]\n",
    "        all_classes[n_id] = word\n",
    "    for key, value in id_dict.items():\n",
    "        result[value] = (key, all_classes[key])\n",
    "    return result\n",
    "\n",
    "def get_data(id_dict):\n",
    "    print('starting loading data')\n",
    "    train_data, test_data = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "    t = time.time()\n",
    "\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [nd.imread( path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open( path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(nd.imread( path + 'val/images/{}'.format(img_name) ,pilmode='RGB'))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
    "print( \"train data shape: \",  train_data.shape )\n",
    "print( \"train label shape: \", train_labels.shape )\n",
    "print( \"test data shape: \",   test_data.shape )\n",
    "print( \"test_labels.shape: \", test_labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6r9ukX807ZEM"
   },
   "outputs": [],
   "source": [
    "# Create context using GMM predictions\n",
    "component_train = read_list(\"gmm_imagenet_train\")\n",
    "component_test = read_list(\"gmm_imagenet_test\")\n",
    "context_train = [[0]*CFG.num_contexts for _ in range(len(component_train))]\n",
    "context_test = [[0]*CFG.num_contexts for _ in range(len(component_test))]\n",
    "context_train = np.array(context_train)\n",
    "context_test = np.array(context_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DDNpbfZ8OXp"
   },
   "outputs": [],
   "source": [
    "# Create our data generator\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\" Helper to iterate over the data (as Numpy arrays). \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img, target_img, context_img, mean, std, number_classes, num_contexts):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img = input_img\n",
    "        self.target_img = target_img\n",
    "        self.context_img = context_img\n",
    "        self.mean = mean\n",
    "        self.number_classes = number_classes\n",
    "        self.std = std\n",
    "        self.num_contexts = num_contexts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns tuple (input, target) correspond to batch #idx. \"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img = self.input_img[i : i + self.batch_size]\n",
    "        batch_target_img = self.target_img[i : i + self.batch_size]\n",
    "        batch_context_img = self.context_img[i:i+self.batch_size]\n",
    "\n",
    "\n",
    "        # images\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, image in enumerate(batch_input_img):\n",
    "          x[j] = image\n",
    "        x = x/255.0\n",
    "        x = (x - self.mean)/self.std\n",
    "\n",
    "        # labels\n",
    "        y = np.zeros((self.batch_size, self.number_classes))\n",
    "        for j, target in enumerate(batch_target_img):\n",
    "          y[j] = target\n",
    "\n",
    "        # Context\n",
    "        contexts = np.zeros((self.batch_size,)+(self.num_contexts, ), dtype=\"int32\")\n",
    "        for j, context in enumerate(batch_context_img):\n",
    "          contexts[j] = context\n",
    "\n",
    "        return (x, contexts), y\n",
    "\n",
    "idx = np.random.permutation(len(train_data))\n",
    "x_train, y_train = train_data[idx], train_labels[idx]\n",
    "context_train = context_train[idx]\n",
    "idx = np.random.permutation(len(test_data))\n",
    "x_test, y_test = test_data[idx], test_labels[idx]\n",
    "context_test = context_test[idx]\n",
    "mean=(0.485, 0.456, 0.406)\n",
    "std=(0.229, 0.224, 0.225)\n",
    "train_data, train_labels, test_data, test_labels\n",
    "x_val = x_train[92000:]\n",
    "y_val = y_train[92000:]\n",
    "context_val = context_train[92000:]\n",
    "train_generator = DataGenerator(CFG.batch_size, (CFG.image_size, CFG.image_size), x_train[:92000], y_train[:92000], context_train[:92000], mean, std, CFG.num_classes, CFG.num_contexts)\n",
    "test_generator = DataGenerator(CFG.batch_size, (CFG.image_size, CFG.image_size), x_test, y_test, context_test, mean, std, CFG.num_classes, CFG.num_contexts)\n",
    "val_generator = DataGenerator(CFG.batch_size, (CFG.image_size, CFG.image_size), x_val, y_val, context_val, mean, std, CFG.num_classes, CFG.num_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqhmJydZ7HR_"
   },
   "outputs": [],
   "source": [
    "# Build our model\n",
    "def build_cnn(num_classes=CFG.num_classes, num_contexts=CFG.num_contexts, learning_rate=CFG.learning_rate, weight_decay=CFG.weight_decay):\n",
    "    input_image = tf.keras.layers.Input(shape=(64,64,3))\n",
    "    context_id = tf.keras.layers.Input(shape=(num_contexts,))\n",
    "    augmented = data_augmentation(input_image)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(augmented)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    pool1 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool1)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool2)\n",
    "    conv3 = ContextNormalization()([input_image, context_id])\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1),  padding=\"same\")(pool3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.ReLU()(conv4)\n",
    "\n",
    "    pool4 = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(conv4)\n",
    "    flattened = tf.keras.layers.Flatten()(pool4)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, name=\"logits\")(flattened)\n",
    "\n",
    "    model = tf.keras.models.Model([input_image, context_id], outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            Precision(name=\"precision\"),\n",
    "            Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8AMohGl8xT5"
   },
   "outputs": [],
   "source": [
    "# Train our model with  context normalization\n",
    "class CustomValidationCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            self.model.load_weights(\"model.ckpt\")\n",
    "            _, accuracy, top_5_accuracy, precision, recall, f1 = self.model.evaluate(test_generator)\n",
    "            print(f\"Test accuracy at epoch {epoch + 1}: {round(accuracy * 100, 2)}%\")\n",
    "            print(f\"Test top 5 accuracy at epoch {epoch + 1}: {round(top_5_accuracy * 100, 2)}%\")\n",
    "            print(f\"Precision at epoch {epoch + 1}: {round(precision * 100, 2)}%\")\n",
    "            print(f\"Recall at epoch {epoch + 1}: {round(recall * 100, 2)}%\")\n",
    "            print(f\"F1-score at epoch {epoch + 1}: {f1}%\")\n",
    "\n",
    "def run_model(model, filepath, batch_size=CFG.batch_size, num_epochs=CFG.num_epochs):\n",
    "    checkpoint_filepath = filepath\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data = val_generator,\n",
    "        callbacks=[checkpoint_callback, CustomValidationCallback()],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = build_cnn()\n",
    "  history = run_model(model, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpHmDvVuqjvu"
   },
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PxzmY5WqqeM"
   },
   "outputs": [],
   "source": [
    "# See model generalization (loss and validation loss)\n",
    "loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
