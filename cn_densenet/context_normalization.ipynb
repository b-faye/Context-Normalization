{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install and Load Packages**"
      ],
      "metadata": {
        "id": "MXZY3HasUgKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow-addons\n",
        "! pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "cqaK6gs7UsNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, AveragePooling2D, concatenate\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, BatchNormalization, ReLU, Concatenate, AveragePooling2D\n",
        "# Import the context normalization layer\n",
        "import os\n",
        "import sys\n",
        "package_dir = os.getcwd()\n",
        "root_dir = os.path.dirname(package_dir)\n",
        "sys.path.append(root_dir)\n",
        "from normalization.layers import ContextNormalization"
      ],
      "metadata": {
        "id": "SmtXMx_nUvMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions for standard normaization and save metrics and loss**"
      ],
      "metadata": {
        "id": "FXQTVFmcU3Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Mean and Standard Deviation\n",
        "def compute_mean_std(dataset):\n",
        "    data_r = np.dstack([dataset[i][:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.dstack([dataset[i][:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.dstack([dataset[i][:, :, 2] for i in range(len(dataset))])\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "9hpRtJhCVA4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save list to binary file\n",
        "def write_list(a_list, file_name):\n",
        "    # store list in binary file so 'wb' mode\n",
        "    with open(file_name, 'wb') as fp:\n",
        "        pickle.dump(a_list, fp)\n",
        "        print('Done writing list into a binary file')\n",
        "\n",
        "# Read list to memory\n",
        "def read_list(file_name):\n",
        "    # for reading also binary mode is important\n",
        "    with open(file_name, 'rb') as fp:\n",
        "        n_list = pickle.load(fp)\n",
        "        return n_list"
      ],
      "metadata": {
        "id": "EVlt9tOqVEPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define General DenseNet Architecture**"
      ],
      "metadata": {
        "id": "kPUXp2rrVKmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_block(x, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "    for _ in range(num_layers):\n",
        "        y = BatchNormalization()(x)\n",
        "        y = ReLU()(y)\n",
        "        y = Conv2D(bn_size * growth_rate, kernel_size=1, strides=1, use_bias=False)(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = ReLU()(y)\n",
        "        y = Conv2D(growth_rate, kernel_size=3, strides=1, padding='same', use_bias=False)(y)\n",
        "        if drop_rate > 0:\n",
        "            y = tf.keras.layers.Dropout(drop_rate)(y)\n",
        "        x = Concatenate()([x, y])\n",
        "        num_input_features += growth_rate\n",
        "    return x, num_input_features\n",
        "\n",
        "def transition(x, num_input_features, num_output_features):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(num_output_features, kernel_size=1, strides=1, use_bias=False)(x)\n",
        "    x = AveragePooling2D(pool_size=2, strides=2)(x)\n",
        "    return x, num_output_features // 2\n",
        "\n",
        "def DenseNet(num_layers, growth_rate=12, num_classes=100, num_contexts=5):\n",
        "    input_tensor = Input(shape=(32, 32, 3))\n",
        "    context_id = tf.keras.layers.Input(shape=(num_contexts,), dtype='int32')\n",
        "    norm = ContextNormalization()([input_tensor, context_id])\n",
        "    x = Conv2D(2 * growth_rate, kernel_size=3, strides=1, padding='same', use_bias=False)(norm)\n",
        "    num_features = 2 * growth_rate\n",
        "\n",
        "    for i in range(3):\n",
        "        x, num_features = dense_block(x, num_layers, num_features, 4, growth_rate, 0.2)\n",
        "        if i < 2:\n",
        "            x, num_features = transition(x, num_features, num_features // 2)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[input_tensor, context_id], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "I4UKCaCpVPEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DenseNet-40**"
      ],
      "metadata": {
        "id": "dLlUnVpChpzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DenseNet with 40 Conv layers\n",
        "model_densenet40 = DenseNet(6)"
      ],
      "metadata": {
        "id": "wW_pSYAphsw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the models with Nesterov's accelerated gradient, weight decay, and momentum\n",
        "num_classes = 100\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "sgd = tf.keras.optimizers.SGD(0.1,momentum=0.9, nesterov=True,weight_decay=1e-4)\n",
        "model_densenet40.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=[\n",
        "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\"),\n",
        "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "zgtqif4xh8hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch > 100 and epoch < 150:\n",
        "        return 0.01\n",
        "    elif epoch > 150:\n",
        "        return 0.001\n",
        "    return 0.1"
      ],
      "metadata": {
        "id": "gUUiy3Ikh_DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset and perform data augmentation\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "AnRkUCRyiCYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess CIFAR-100 dataset\n",
        "mean, std = compute_mean_std(x_train)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = (x_test - mean) / std"
      ],
      "metadata": {
        "id": "UpFWaT4miPPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build contexts\n",
        "num_contexts = 5\n",
        "context_path_tr = \"../gmm/gmm_cifar100_tr_labels_k_5\"\n",
        "context_path_ts = \"../gmm/gmm_cifar100_ts_labels_k_5\"\n",
        "component_train = read_list(context_path_tr)\n",
        "component_test = read_list(context_path_ts)\n",
        "\n",
        "context_train = [[0]*num_contexts for _ in range(len(component_train))]\n",
        "context_test = [[0]*num_contexts for _ in range(len(component_test))]\n",
        "\n",
        "for iter, label in enumerate(component_train):\n",
        "    context_train[iter][label] = 1\n",
        "\n",
        "for iter, label in enumerate(component_test):\n",
        "    context_test[iter][label] = 1\n",
        "\n",
        "context_train = np.array(context_train)\n",
        "context_test = np.array(context_test)"
      ],
      "metadata": {
        "id": "HlJuaNT2iZAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generator\n",
        "train_size = 40000\n",
        "x_val = x_train[train_size:]\n",
        "y_val = y_train[train_size:]\n",
        "context_val = context_train[train_size:]\n",
        "\n",
        "x_train = x_train[:train_size]\n",
        "y_train = y_train[:train_size]\n",
        "context_train = context_train[:train_size]\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "train_gen = datagen.flow(x=(x_train, context_train), y=y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "otuhhScZiUkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_EPOCH = x_train.shape[0] // batch_size\n",
        "SAVE_PERIOD = 50"
      ],
      "metadata": {
        "id": "EtrTFautj960"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to  save checkpoints\n",
        "checkpoint_dir = '.'\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'model_weights_{epoch:03d}.h5'),\n",
        "    save_weights_only=True,\n",
        "    save_best_only=False,\n",
        "    save_freq=SAVE_PERIOD*STEPS_PER_EPOCH\n",
        ")\n",
        "\n",
        "# Train the DenseNet-40 model\n",
        "history = model_.fit(train_gen,\n",
        "                     steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
        "                     validation_data=((x_val, context_val), y_val), callbacks=[keras.callbacks.LearningRateScheduler(lr_schedule), checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "e29xDCv7kGB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy, top_5_accuracy, precision, recall, f1 = model_.evaluate((x_test, context_test), y_test)\n",
        "print(f\"Test loss : {loss}\")\n",
        "print(f\"Test accuracy : {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy : {round(top_5_accuracy * 100, 2)}%\")\n",
        "print(f\"Precision : {round(precision * 100, 2)}%\")\n",
        "print(f\"Recall : {round(recall * 100, 2)}%\")\n",
        "print(f\"F1-score : {f1}%\")"
      ],
      "metadata": {
        "id": "_1-SKmtLkVlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DenseNet-100**"
      ],
      "metadata": {
        "id": "j1mLbvBjkimX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DenseNet with 100 Conv layers\n",
        "model_densenet100 = DenseNet(16)"
      ],
      "metadata": {
        "id": "Zs1s3nXikimY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the models with Nesterov's accelerated gradient, weight decay, and momentum\n",
        "num_classes = 100\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "sgd = tf.keras.optimizers.SGD(0.1,momentum=0.9, nesterov=True,weight_decay=1e-4)\n",
        "model_densenet100.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=[\n",
        "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\"),\n",
        "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "RDbMpulmkimZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch > 100 and epoch < 150:\n",
        "        return 0.01\n",
        "    elif epoch > 150:\n",
        "        return 0.001\n",
        "    return 0.1"
      ],
      "metadata": {
        "id": "HRoeojUPkima"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset and perform data augmentation\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "8RKbnr5Ykimc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess CIFAR-100 dataset\n",
        "mean, std = compute_mean_std(x_train)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = (x_test - mean) / std"
      ],
      "metadata": {
        "id": "o7q-ljjgkimd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build contexts\n",
        "num_contexts = 5\n",
        "context_path_tr = \"../gmm/gmm_cifar100_tr_labels_k_5\"\n",
        "context_path_ts = \"../gmm/gmm_cifar100_ts_labels_k_5\"\n",
        "component_train = read_list(context_path_tr)\n",
        "component_test = read_list(context_path_ts)\n",
        "\n",
        "context_train = [[0]*num_contexts for _ in range(len(component_train))]\n",
        "context_test = [[0]*num_contexts for _ in range(len(component_test))]\n",
        "\n",
        "for iter, label in enumerate(component_train):\n",
        "    context_train[iter][label] = 1\n",
        "\n",
        "for iter, label in enumerate(component_test):\n",
        "    context_test[iter][label] = 1\n",
        "\n",
        "context_train = np.array(context_train)\n",
        "context_test = np.array(context_test)"
      ],
      "metadata": {
        "id": "xX0qgDctkime"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generator\n",
        "train_size = 40000\n",
        "x_val = x_train[train_size:]\n",
        "y_val = y_train[train_size:]\n",
        "context_val = context_train[train_size:]\n",
        "\n",
        "x_train = x_train[:train_size]\n",
        "y_train = y_train[:train_size]\n",
        "context_train = context_train[:train_size]\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "train_gen = datagen.flow(x=(x_train, context_train), y=y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "7Zrd1rcIkimf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_EPOCH = x_train.shape[0] // batch_size\n",
        "SAVE_PERIOD = 50"
      ],
      "metadata": {
        "id": "JqFMtxgikimf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to  save checkpoints\n",
        "checkpoint_dir = '.'\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'model_weights_{epoch:03d}.h5'),\n",
        "    save_weights_only=True,\n",
        "    save_best_only=False,\n",
        "    save_freq=SAVE_PERIOD*STEPS_PER_EPOCH\n",
        ")\n",
        "\n",
        "# Train the DenseNet-100 model\n",
        "history = model_densenet100.fit(train_gen,\n",
        "                     steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
        "                     validation_data=((x_val, context_val), y_val), callbacks=[keras.callbacks.LearningRateScheduler(lr_schedule), checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "CN5ocozTkimf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy, top_5_accuracy, precision, recall, f1 = model_densenet100.evaluate((x_test, context_test), y_test)\n",
        "print(f\"Test loss : {loss}\")\n",
        "print(f\"Test accuracy : {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy : {round(top_5_accuracy * 100, 2)}%\")\n",
        "print(f\"Precision : {round(precision * 100, 2)}%\")\n",
        "print(f\"Recall : {round(recall * 100, 2)}%\")\n",
        "print(f\"F1-score : {f1}%\")"
      ],
      "metadata": {
        "id": "fjYImuASkimg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
